{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n",
      "tensor([1, 2, 3], device='mps:0') mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1,2,3])# by default this exists on the cpu , we need to send it to the gpu \n",
    "print(tensor,tensor.device)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "print(tensor_on_gpu, tensor_on_gpu.device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#now we can see that the since that the  numpy is on the gpu , since the computation is done on cpu and not on the gpu , this will lead to an error \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tensor_on_gpu\u001b[39m.\u001b[39;49mnumpy()\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "#now we can see that the since that the  numpy is on the gpu , since the computation is done on cpu and not on the gpu , this will lead to an error \n",
    "tensor_on_gpu.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#now lets send the tensor on gpu to the cpu \n",
    "tensor_on_cpu = tensor_on_gpu.cpu()\n",
    "\n",
    "print(tensor_on_cpu.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3950, 0.4549, 0.8235, 0.4352, 0.3722, 0.0297, 0.2472],\n",
      "        [0.0298, 0.4832, 0.9539, 0.3452, 0.0529, 0.6992, 0.6587],\n",
      "        [0.3990, 0.5580, 0.8735, 0.8691, 0.7384, 0.5120, 0.1090],\n",
      "        [0.6257, 0.0635, 0.6094, 0.1310, 0.5773, 0.5507, 0.3645],\n",
      "        [0.9897, 0.8829, 0.2612, 0.6100, 0.8672, 0.8960, 0.6690],\n",
      "        [0.0626, 0.8196, 0.5329, 0.0824, 0.9255, 0.2051, 0.1434],\n",
      "        [0.3835, 0.6640, 0.7996, 0.1196, 0.1255, 0.3602, 0.2206]])\n",
      "torch.Size([1, 49])\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "random_tensor = torch.rand(7,7)\n",
    "\n",
    "print(random_tensor)\n",
    "\n",
    "#now lets create a view of the numpy array\n",
    "\n",
    "z= random_tensor.view(1,49)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'accelerator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m      2\u001b[0m     tensor\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mcurrent_accelerator())\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'accelerator'"
     ]
    }
   ],
   "source": [
    "if torch.accelerator.is_available():\n",
    "    tensor=tensor.to(torch.accelerator.current_accelerator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "\n",
    "X= torch.arange(0,1,0.02).unsqueeze(dim=1)\n",
    "\n",
    "y= weight*X+ bias\n",
    "\n",
    "X[:10] , y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 10 40 10\n"
     ]
    }
   ],
   "source": [
    "#we now seperate the datasets into the train ,test and split.\n",
    "\n",
    "train_split = int(0.8*len(X))\n",
    "\n",
    "x_train, y_train=X[:train_split],y[:train_split]\n",
    "\n",
    "x_test,y_test = X[train_split:],y[train_split:] \n",
    "\n",
    "print(len(x_train),len(x_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([0.3367], requires_grad=True), Parameter containing:\n",
      "tensor([0.1288], requires_grad=True)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, dtype=torch.float32, requires_grad=True))\n",
    "        self.bias = nn.Parameter(torch.randn(1, dtype=torch.float32, requires_grad=True))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.weights * x + self.bias   # fixed \"self.weight\" typo to \"self.weights\"\n",
    "    \n",
    "# Set manual seed since nn.Parameter are randomly initialized\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create an instance of the model\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Check parameters\n",
    "print(list(model_0.parameters()))\n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the testing samples are  10\n",
      "Number of training samples are 10\n",
      "Predicted values are tensor([[0.3982],\n",
      "        [0.4049],\n",
      "        [0.4116],\n",
      "        [0.4184],\n",
      "        [0.4251],\n",
      "        [0.4318],\n",
      "        [0.4386],\n",
      "        [0.4453],\n",
      "        [0.4520],\n",
      "        [0.4588]])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    y_preds = model_0(x_test)\n",
    "    \n",
    "    \n",
    "#now lets see the predictions we get \n",
    "print(f\"Number of the testing samples are  {len(x_test)}\")\n",
    "\n",
    "print(f\"Number of training samples are {len(y_preds)}\")\n",
    "\n",
    "print(f\"Predicted values are {y_preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11f497f40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsN0lEQVR4nO3df3DU9YH/8ddmIRvpAeqlJIFsuyOeotWSa5BctNTkJi0dHS5OxpMzLXBcxTukTmJ6dyVViYcncarnwSgt1xycTH8Ildn2OoVJ9VIzh5UbbkBmbI34pQSJkQSYKsnFa6Kf/Xz/+NwGluwm+9ns7mc/n30+ZnZyfPh8Nu98LnVfvD/vz+vjM03TFAAAgEMKnB4AAADIb4QRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICjZjg9gGREIhG99957mj17tnw+n9PDAQAASTBNU8PDw5o/f74KChLPf7gijLz33nsKBoNODwMAAKSgr69P5eXlCf/eFWFk9uzZkqwfZs6cOQ6PBgAAJGNoaEjBYHD8czwRV4SR6KWZOXPmEEYAAHCZqZZYsIAVAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICjCCMAAMBRhBEAAOAowggAAHCUK0rPAAB5wjCkgwelM2eksjJp2TLJ73d6VMgwwggAIDeEw1JTk/Tuuxe3lZdL27ZJDQ3OjQsZx2UaAIDzwmHp7rtjg4gk9fdb28NhZ8aFrCCMAACcZRjWjIhpTvy76LbmZms/eBJhBADgrIMHJ86IXMo0pb4+az94EmEEAOCsM2fSux9chzACAHBWWVl694PrEEYAAM5atsy6a8bni//3Pp8UDFr7wZMIIwAAZ/n91u270sRAEv3z1q30jXgYYQQAkB2GIXV3Sy+8YH299O6YhgZp3z5pwYLYY8rLre30jHgapWcAgMxLptCsoUGqr6eBNQ/5TDPejd25ZWhoSHPnztWFCxc0Z84cp4cDALAjWmh2+cdN9BIMMx+eleznN5dpAACZQ6EZkkAYAQBkDoVmSAJhBACQORSaIQmEEQBA5lBohiQQRgAAmUOhGZJAGAEAZA6FZkgCYQQAMH0UmmEaKD0DAEwPhWaYJkrPAACpo9AMk6D0DACQWRSaIU0IIwCA1FBohjQhjAAAUkOhGdKEMAIASA2FZkgTwggAIDUUmiFNCCMAgMkl6hCh0AxpQhgBACQWDkuhkFRbKzU2Wl9DIWu7RKEZ0oKeEQBAfHY6RAyDQjNMkOznN2EEADCRYVgzIIlu3fX5rNmP3l5CBxLKaOnZ9u3bFQqFVFRUpKqqKh0+fHjS/bdu3arrr79eV1xxhYLBoB566CH9/ve/T+VbAwCygQ4RZJHtMLJ37161tLSora1NR48e1eLFi7V8+XKdPXs27v4/+tGPtHHjRrW1tamnp0c7d+7U3r179a1vfWvagwcAZAgdIsgi22HkmWee0bp167R27VrdeOON2rFjh2bNmqVdu3bF3f+1117TbbfdpsbGRoVCIX3pS1/SvffeO+VsCgDAQXSIIItshZGxsTEdOXJEdXV1F9+goEB1dXU6dOhQ3GNuvfVWHTlyZDx8nDx5UgcOHNAdd9yR8PuMjo5qaGgo5gUAyCI6RJBFtsLI+fPnZRiGSkpKYraXlJRoYGAg7jGNjY3avHmzPv/5z2vmzJlauHChampqJr1M097errlz546/gsGgnWECAKaLDhFkUcZ7Rrq7u7VlyxZ95zvf0dGjRxUOh7V//349/vjjCY9pbW3VhQsXxl99fX2ZHiYA5J9EZWZRdIggS2bY2bm4uFh+v1+Dg4Mx2wcHB1VaWhr3mEcffVSrVq3SfffdJ0m6+eabNTIyovvvv18PP/ywCgom5qFAIKBAIGBnaAAAO8Jhqakp9o6Z8nJrNuTSkNHQINXX0yGCjLI1M1JYWKjKykp1dXWNb4tEIurq6lJ1dXXcYz788MMJgcP/f7/ELqg4AQDviZaZXX7rbn+/tT3arhrl90s1NdK991pfCSJIM9uXaVpaWtTR0aHdu3erp6dH69ev18jIiNauXStJWr16tVpbW8f3X7Fihb773e9qz5496u3t1csvv6xHH31UK1asGA8lAIAsMQxrRiTePwaj25qbJ16yATLI1mUaSVq5cqXOnTunTZs2aWBgQBUVFers7Bxf1Hr69OmYmZBHHnlEPp9PjzzyiPr7+/XJT35SK1as0BNPPJG+nwIAkBw7ZWY1NVkbFvIbdfAAkE9eeMF64N1UfvQj67IMMA0ZrYMHALgUZWbIQYQRAMgnlJkhBxFGACCfUGaGHEQYAQAvmqzQjDIz5Bjbd9MAAHJcMoVmlJkhh3A3DQB4SbTQ7PL/tEcvwTDzgSzibhoAyDcUmsGlCCMA4BV2Cs2AHEIYAQCvOHMmvfsBWUIYAQCvoNAMLkUYAQCvoNAMLkUYAQC3SdQhQqEZXIowAgBuEg5LoZBUW2s98K621vpzOGz9PYVmcCF6RgDALex0iBgGhWZwXLKf34QRAHADw7BmQBLduuvzWbMfvb2EDuQMSs8AwEvoEIGHEUYAwA3oEIGHEUYAwA3oEIGHEUYAwA3oEIGHEUYAwA3oEIGHEUYAIJckKjST6BCBZ81wegAAgP8TDktNTbF3zZSXWzMi0aDR0CDV19MhAk+hZwQAcoGdQjPAJegZAQC3MAxrRiTevw2j25qbYy/ZAB5CGAEAp1FohjxHGAEAp1FohjxHGAEAp1FohjxHGAEAp1FohjxHGAGAbEnUIUKhGfIcYQQAsiEclkIhqbZWamy0voZC1naJQjPkNXpGACDT7HSIGAaFZvCMZD+/CSMAkEmGYc2AJLp11+ezZj96ewkd8BxKzwAgF9AhAkyJMAIAmUSHCDAlwggAZBIdIsCUCCMAkEl0iABTIowAQCbRIQJMiTACANOVqMwsig4RYFIznB4AALhaOCw1NcXeMVNebs2GXBoyGhqk+no6RIA46BkBgFTZKTMD8hA9IwCQSYZhzYjE+/dcdFtz88RLNgAmIIwAQCooMwPShjACAKmgzAxIG8IIAKSCMjMgbQgjAJAKysyAtCGMAEAqKDMD0oYwAgCTmazQjDIzIC0oPQOARJIpNKPMDJg2Ss8AIB4KzYBpo/QMAFJFoRmQVSmFke3btysUCqmoqEhVVVU6fPhwwn1ramrk8/kmvO68886UBw0AGUWhGZBVtsPI3r171dLSora2Nh09elSLFy/W8uXLdfbs2bj7h8NhnTlzZvz161//Wn6/X3/+538+7cEDQEZQaAZkle0w8swzz2jdunVau3atbrzxRu3YsUOzZs3Srl274u5/9dVXq7S0dPz18ssva9asWYQRALmLQjMgq2yFkbGxMR05ckR1dXUX36CgQHV1dTp06FBS77Fz5079xV/8hT7xiU8k3Gd0dFRDQ0MxLwDIGgrNgKyyFUbOnz8vwzBUUlISs72kpEQDAwNTHn/48GH9+te/1n333Tfpfu3t7Zo7d+74KxgM2hkmACQnUYcIhWZAVmX1bpqdO3fq5ptv1tKlSyfdr7W1VRcuXBh/9fX1ZWmEAPJGOCyFQlJtrdTYaH0NhaztEoVmQBbZKj0rLi6W3+/X4OBgzPbBwUGVlpZOeuzIyIj27NmjzZs3T/l9AoGAAoGAnaEBQPISdYj091vbo2GDQjMgK2zNjBQWFqqyslJdXV3j2yKRiLq6ulRdXT3psS+++KJGR0f11a9+NbWRAkA62O0Q8fulmhrp3nutrwQRIO1sX6ZpaWlRR0eHdu/erZ6eHq1fv14jIyNau3atJGn16tVqbW2dcNzOnTt111136Q//8A+nP2oASBUdIkDOsf1smpUrV+rcuXPatGmTBgYGVFFRoc7OzvFFradPn1ZBQWzGOX78uF599VW99NJL6Rk1AKSKDhEg5/BsGgD5pbvbWqw6lVdesS7LAEgZz6YBgHjoEAFyDmEEQH6hQwTIOYQRAN6UqNBMokMEyDG2F7ACQM4Lh63bdy+9a6a83JoRiQYNOkSAnMECVgDekqjQLHoJhpkPIGtYwAog/9gtNAOQEwgjALyDQjPAlQgjALyDQjPAlQgjALyjrCy9+wHICsIIAO+g0AxwJcIIAPdJ1CFCoRngSoQRAO4SDkuhkPV8mcZG62soZG2XKDQDXIieEQDuYadDxDAoNAMcluznN2EEgDsYhjUDkujWXZ/Pmv3o7SV0ADmC0jMA3kKHCOBZhBEA7kCHCOBZhBEA7kCHCOBZhBEA7kCHCOBZhBEA7kCHCOBZhBEAuSNRmVkUHSKAJ81wegAAIMnqEGlqir1jprzcmg25NGQ0NEj19XSIAB5CzwgA59kpMwPgGvSMAHAHw7BmROL9uyi6rbl54iUbAJ5BGAHgLMrMgLxHGAHgLMrMgLxHGAHgLMrMgLxHGAHgLMrMgLxHGAHgLMrMgLxHGAGQHZMVmlFmBuQ1Ss8AZF4yhWaUmQF5i9IzAJlFoRmQtyg9A+A8Cs0AJIEwAiBzKDQDkATCCIDModAMQBIIIwAyh0IzAEkgjADIHArNACSBMAJg+hJ1iFBoBiAJhBEA0xMOS6GQVFsrNTZaX0Mha7tEoRmAKdEzAiB1djpEDINCMyDPJPv5TRgBkBrDsGZAEt266/NZsx+9vYQOIE9RegYgs+gQAZAmhBEAqaFDBECaEEYApIYOEQBpQhgBkBo6RACkCWEEQGroEAGQJoQRAJNLVGgm0SECIC1mOD0AADksHJaammLvmikvt2ZEokGjoUGqr6dDBEDK6BkBEJ+dQjMAiIOeEQCpMwxrRiTev1Wi25qbYy/ZAECKCCMAJqLQDEAWEUYATEShGYAsSimMbN++XaFQSEVFRaqqqtLhw4cn3f+DDz7Qhg0bVFZWpkAgoOuuu04HDhxIacAAsoBCMwBZZDuM7N27Vy0tLWpra9PRo0e1ePFiLV++XGfPno27/9jYmL74xS/q1KlT2rdvn44fP66Ojg4tuPxWQAC5g0IzAFlk+26aqqoq3XLLLXruueckSZFIRMFgUA8++KA2btw4Yf8dO3boqaee0ltvvaWZM2emNEjupgEcEL2bRopdyMrdNACSlJG7acbGxnTkyBHV1dVdfIOCAtXV1enQoUNxj/nZz36m6upqbdiwQSUlJbrpppu0ZcsWGZOswh8dHdXQ0FDMC0AGUGgGIAfYKj07f/68DMNQSUlJzPaSkhK99dZbcY85efKkfvnLX+orX/mKDhw4oBMnTuiBBx7QRx99pLa2trjHtLe36x/+4R/sDA2AXRSaAcgRGW9gjUQimjdvnr73ve/J7/ersrJS/f39euqppxKGkdbWVrW0tIz/eWhoSMFgMNNDBfJHokKz/n5r+6UzH36/VFOT9SECyB+2wkhxcbH8fr8GBwdjtg8ODqq0tDTuMWVlZZo5c6b8l/xL6oYbbtDAwIDGxsZUWFg44ZhAIKBAIGBnaACSNVWhmc9nFZrV1zMDAiArbK0ZKSwsVGVlpbq6usa3RSIRdXV1qbq6Ou4xt912m06cOKFIJDK+7e2331ZZWVncIAIgwyg0A5BjbN/a29LSoo6ODu3evVs9PT1av369RkZGtHbtWknS6tWr1draOr7/+vXr9bvf/U5NTU16++23tX//fm3ZskUbNmxI308BIHkUmgHIMbbXjKxcuVLnzp3Tpk2bNDAwoIqKCnV2do4vaj19+rQKCi5mnGAwqF/84hd66KGH9NnPflYLFixQU1OTvvnNb6bvpwCQPArNAOQYntoL5BvDkEIha7FqvP/5+3zWXTW9vawZATAtPLUXyGeT9Yf4/dbtu9LEhtXon7duJYgAyBrCCOA14bA181FbKzU2Wl9DIWt7FIVmAHIIl2kAL0nUH5Kowt0wKDQDkDHJfn4TRgCviK4FSXTbLmtBAGQZa0aAfEN/CACXIowAXkF/CACXIowAXkF/CACXIowAXrFsmbUm5PLbdaN8PikYtPYDgBxCGAG8gv4QAC5FGAHcZrJCM/pDALiQ7WfTAHBQOCw1NcXeNVNebs2IRINGQ4NUX09/CADXoGcEcAu7hWYA4DB6RgAvMQxrRiTevx2i25qbYy/ZAIBLEEYAN6DQDICHEUYAN6DQDICHEUYAN6DQDICHEUYAN6DQDICHEUaAXJKoQ4RCMwAeRhgBckU4LIVCUm2t1NhofQ2FrO0ShWYAPIueESAX2OkQMQwKzQC4QrKf34QRwGmGYc2AJLp11+ezZj96ewkdAFyF0jPALegQAZDnCCOA0+gQAZDnCCOA0+gQAZDnCCOA0+gQAZDnCCOA0+gQAZDnCCNAtiQqNJPoEAGQ12Y4PQAgL4TDUlNT7F0z5eXWjEg0aDQ0SPX1dIgAyDv0jACZZqfQDAA8hJ4RIBcYhjUjEi/zR7c1N8desgGAPEMYATKJQjMAmBJhBMgkCs0AYEqEESCTKDQDgCkRRoBMotAMAKZEGAEyiUIzAJgSYQRIBwrNACBllJ4B00WhGQBMC6VnwHRQaAYACVF6BmQahWYAkBaEESBVFJoBQFoQRoBUUWgGAGlBGAFSRaEZAKQFYQRIFYVmAJAWhBFgMpP1h1BoBgBpQRgBEgmHpVBIqq2VGhutr6GQtT2KQjMAmDZ6RoB47PaHGAaFZgBwmWQ/vwkjwOUMw5oBSXTbrs9nzXz09hI4AGASlJ4BqaI/BACyijACXI7+EADIKsIIcDn6QwAgq1IKI9u3b1coFFJRUZGqqqp0+PDhhPs+//zz8vl8Ma+ioqKUBwxkHP0hAJBVtsPI3r171dLSora2Nh09elSLFy/W8uXLdfbs2YTHzJkzR2fOnBl/vfPOO9MaNJBR9IcAQFbZDiPPPPOM1q1bp7Vr1+rGG2/Ujh07NGvWLO3atSvhMT6fT6WlpeOvkpKSaQ0aSIvJCs3oDwGArLEVRsbGxnTkyBHV1dVdfIOCAtXV1enQoUMJj/uf//kfffrTn1YwGFR9fb1+85vfTPp9RkdHNTQ0FPMC0irZQrNTp6RXXpF+9CPra28vQQQA0sxWGDl//rwMw5gws1FSUqKBgYG4x1x//fXatWuX/v3f/10/+MEPFIlEdOutt+rdSW6dbG9v19y5c8dfwWDQzjCByUULzS7/Hezvt7ZfGkj8fqmmRrr3Xusrl2YAIO0yfjdNdXW1Vq9erYqKCt1+++0Kh8P65Cc/qX/5l39JeExra6suXLgw/urr68v0MJEvDENqaprYrCpd3NbcHHvJBgCQUTPs7FxcXCy/36/BwcGY7YODgyotLU3qPWbOnKk//uM/1okTJxLuEwgEFAgE7AwNSI6dQrOamqwNCwDyma2ZkcLCQlVWVqqrq2t8WyQSUVdXl6qrq5N6D8Mw9MYbb6iMjgY4gUIzAMg5tmZGJKmlpUVr1qzRkiVLtHTpUm3dulUjIyNau3atJGn16tVasGCB2tvbJUmbN2/Wn/zJn+jaa6/VBx98oKeeekrvvPOO7rvvvvT+JEAyKDQDgJxjO4ysXLlS586d06ZNmzQwMKCKigp1dnaOL2o9ffq0CgouTri8//77WrdunQYGBnTVVVepsrJSr732mm688cb0/RRAsqKFZv398deNRB+CR6EZAGQNT+2FNxmGte7jzBlrlmPZsot3wkTvppFiA0m00IweEQBIC57ai/w1VYcIhWYAkFOYGYG3RGc9Lv+1jjfrMdnsCQBg2pL9/CaMwDsMw5oBSXTrbnQ9SG8voQMAsoDLNMg/djpEAAA5gzAC76BDBABciTAC76BDBABciTAC74h2iEQXq17O55OCQTpEACDHEEbgHX6/tG2b9X9fHkiif966lcWrAJBjCCNwH8OQurulF16wvl76hF06RADAdWzXwQOOCoelpqbYu2bKy60ZkWjQaGiQ6uvpEAEAl6BnBO5hp9AMAOA4ekbgLYZhzYjEy87Rbc3NsZdsAACuQBiBO1BoBgCeRRiBO1BoBgCexQJWuAOFZgCQdkbE0MHTB3Vm+IzKZpdp2aeWyV+Q/cX+hBG4Q7TQrL8//rqR6EPwKDQDgKSEe8Jq6mzSu0MXL4GXzynXti9vU8MN2b0ZgMs0cAcKzQAgbcI9Yd3947tjgogk9Q/16+4f361wTzir4yGMIHdMVmYmUWgGAGlgRAw1dTbJ1MRZ5ui25s5mGZHs3Z3IZRrkhmTKzCQKzQAgSYnWgxw8fXDCjMilTJnqG+rTwdMHVROqycpYCSNwXqIys/5+a/vlsx5+v1RTk9UhAoCbTLYeZPTj0aTe48xw9u5O5DINnEWZGQCk1VTrQf7f7/5fUu9TNjt7dycSRuAsyswAIG2SWQ/ScaRD5bPL5ZNvwj6S5JNPwTlBLftU9u5OJIzAWZSZAYBtRsRQ96luvfDGC+o+1T2+2DSZ9SDvDr+rdZXrJGlCIIn+eeuXt2a1b4Q1I3AWZWYAYEs61oP80dV/pH337Iv7Plu/vDXrPSOEETiLMjMASFp0Pcjll2Gi60Eeq3ksqfcpm12mmlCN6q+vz4kGVp9pxvsEyC3JPoIYOcwwEt+OG72bRooNJNEyMzpEAEBGxFBoWyjhZRiffFow2+ph6h/uj7tuxCefyueUq7epNyuhI9nPb9aMIPPCYSkUkmprpcZG62soZG2XKDMDgEt4bT1IMrhMg8xKtkOEMjMA8OR6kGRwmQaZYxjWDEiiW3ej60F6ewkdAPJeovUg0RmNx2oeU1t325Tv88qaV1QTqsmJJ/Im+/lNGEHmdHdbl2Sm8sorNKoCyGtuXA+SDNaMwHl0iADAuERrQSRvrwdJBmEEmUOHCABIsi7BhLaFVLu7Vo3hRtXurlVoW0jhHmshf7LPgYmuB1kwJ3bBf/mccu27Z19OrgdJBgtYkTl0iADAlN0g++7Zl/RzYHKtHyRdCCPIHL9f2rbNumvG54vfIbJ1K4tXAXjWVM+K8cmn5s5mnXjwhMrnlKt/aPL1INHnxfgL/KoJ1WR6+FnDZRpMn2FYi1VfeMH6eukTdukQAZAHptMN0jfUp9fefU3bvrxNkvfWgySDmRFMTzgsNTXF3r5bXm7NiESDBh0iADwsHd0gZ4bP6N6b73VdP0i6cGsvUpeo0IwadwB5It3dIJJyoh8kXegZQWZRaAYgz3m1GySd6BlBZh08mDiISNZsSV+ftR8AeFC+d4OkE2EEqaHQDECeSLQ4Nd+7QdKJBaxIDYVmAPLAZItT870bJJ1YM4LURNeMTFVoxpoRAC411eLUvXfvVctLLVN2g3h1PUgyWDOCzIoWmkkX756JotAMgMtNVVYmSd946Rv65y/9syTWg0wXYQSTo9AMgIdNt6ys+BPFrAdJA9aMIDEKzQB4WDrLylgPMj2EEcSXqNCsv9/afunMh98v1dRkfYgAkKqpHl73WM1jSb1PdBGr154Vk20sYMVEFJoB8DDKyrKHBaxIHYVmADxgOutBKCvLLi7TYCIKzQC4XDrWg0TLyvLxwXXZRhjBRBSaAXCxdK4HoawsO1gzgokoNAPgUqwHyS0ZXTOyfft2hUIhFRUVqaqqSocPH07quD179sjn8+muu+5K5dsi3RJ1iFBoBiCHJVoLIrEexK1sh5G9e/eqpaVFbW1tOnr0qBYvXqzly5fr7Nmzkx536tQp/e3f/q2WLVuW8mCRRuGwNftRWys1NlpfQyFru0ShGYCcFO4JK7QtpNrdtWoMN6p2d61C20IK91j/7eLhde5k+zJNVVWVbrnlFj333HOSpEgkomAwqAcffFAbN26Me4xhGPrCF76gv/qrv9LBgwf1wQcf6Kc//WnS35PLNGmWqEMkOutxadgwDArNAOSEqZ4Vs++efbr6iqtVu7t2yvd6Zc0rqgnVyIgYrAfJoGQ/v20tYB0bG9ORI0fU2to6vq2goEB1dXU6dOhQwuM2b96sefPm6Wtf+5oOJnE76OjoqEZHL652HhoasjNMTMYwrFbVeBnUNK1A0txstar6/RSaAcgJUz0rxiefmjubdeLBEyqfUz7lw+uWfcqapaesLDfYukxz/vx5GYahkpKSmO0lJSUaGBiIe8yrr76qnTt3qqOjI+nv097errlz546/gsGgnWFiMnSIAHChZJ8V89q7r2nbl601b6wHcY+Mlp4NDw9r1apV6ujoUHFxcdLHtba26sKFC+Ovvr6+DI4yz9AhAsCFkl0Lcmb4jBpuaGA9iMvYukxTXFwsv9+vwcHBmO2Dg4MqLS2dsP9vf/tbnTp1SitWrBjfFolErG88Y4aOHz+uhQsXTjguEAgoEAjYGRqSRYcIABeKPgMm2f0abmigH8RFbIWRwsJCVVZWqqura/z23Egkoq6uLn3961+fsP+iRYv0xhtvxGx75JFHNDw8rG3btnH5xQnLlll3xEzVIcJdTwByyLJPLbO1FkRiPYib2G5gbWlp0Zo1a7RkyRItXbpUW7du1cjIiNauXStJWr16tRYsWKD29nYVFRXppptuijn+yiuvlKQJ25El0Q6Ru++2gselgYQOEQA5yl/g17Yvb9PdP75bPvliAglrQdzP9pqRlStX6umnn9amTZtUUVGhY8eOqbOzc3xR6+nTp3WG9QbOSlRmFkWHCAAXYi2Id1EH7zXhsHXr7qV3zJSXW7Mhl4cMOkQAuBDdIO6R7Oc3YcRL7JSZAQCQYRl9Ng1y0FRlZpJVZnb5JRsAABxGGPEKyswAAC5FGPEKyswAAC5FGPEKyswAAC5FGPGKaJmZzxf/730+KRikzAwAkHMII26TqEMkWmYmTQwklJkBAHIYYcRNwmEpFJJqa6XGRutrKGRtlygzAwC4Ej0jbmGnQ4QyMwBADqD0zEsMw5oBSXTrbvThdr29hA4AQM6g9MxL6BABAHgYYcQN6BABAHgYYcQN6BABAHgYYcQN6BABAHgYYcQN6BABAHgYYSSXJCo0k+gQAQB41gynB4D/Ew5LTU2xd82Ul1szItGg0dAg1dfTIQIA8BR6RnKBnUIzAABcgp4RtzAMa0YkXiaMbmtujr1kAwCAhxBGnEahGQAgzxFGnEahGQAgzxFGnEahGQAgzxFGnEahGQAgzxFGnEahGQAgzxFGsoVCMwAA4qL0LBsoNAMAICFKzzKNQjMAQJ6i9CwXUGgGAMCUCCOZRKEZAABTIoxkEoVmAABMiTCSSRSaAQAwJcJIJlFoBgDAlAgj6ZCoQ4RCMwAApkQYma5wWAqFpNpaqbHR+hoKWdslCs0AAJgCPSPTYadDxDAoNAMA5JVkP78JI6kyDGsGJNGtuz6fNfvR20voAADkJUrPMo0OEQAA0oIwkio6RAAASAvCSKroEAEAIC0II6miQwQAgLQgjKSKDhEAANKCMDKZRGVmUXSIAAAwbTOcHkDOCoelpqbYO2bKy63ZkEtDRkODVF9PhwgAACmiZyQeO2VmAAAgLnpGUmUY1oxIvIwW3dbcPPGSDQAASAlh5HKUmQEAkFWEkctRZgYAQFYRRi5HmRkAAFlFGLkcZWYAAGRV/oaRRB0ilJkBAJBVKYWR7du3KxQKqaioSFVVVTp8+HDCfcPhsJYsWaIrr7xSn/jEJ1RRUaHvf//7KQ84LcJhKRSSamulxkbrayhkbZcoMwMAIIts94zs3btXq1ev1o4dO1RVVaWtW7fqxRdf1PHjxzVv3rwJ+3d3d+v999/XokWLVFhYqJ///Of6xje+of3792v58uVJfc+09ozY6RAxDMrMAABIUbKf37bDSFVVlW655RY999xzkqRIJKJgMKgHH3xQGzduTOo9Pve5z+nOO+/U448/ntT+aQsjhmHNgCS6ddfns2Y/ensJHQAATFNGSs/GxsZ05MgR1dXVXXyDggLV1dXp0KFDUx5vmqa6urp0/PhxfeELX0i43+joqIaGhmJeaUGHCAAAOcdWGDl//rwMw1BJSUnM9pKSEg0MDCQ87sKFC/qDP/gDFRYW6s4779Szzz6rL37xiwn3b29v19y5c8dfwWDQzjATo0MEAICck5W7aWbPnq1jx47pv//7v/XEE0+opaVF3d3dCfdvbW3VhQsXxl99fX3pGQgdIgAA5BxbT+0tLi6W3+/X4OBgzPbBwUGVlpYmPK6goEDXXnutJKmiokI9PT1qb29XTU1N3P0DgYACgYCdoSUn2iHS3x//2TPRNSN0iAAAkDW2ZkYKCwtVWVmprq6u8W2RSERdXV2qrq5O+n0ikYhGR0ftfOv0oEMEAICcY/syTUtLizo6OrR792719PRo/fr1GhkZ0dq1ayVJq1evVmtr6/j+7e3tevnll3Xy5En19PTon/7pn/T9739fX/3qV9P3U9hBhwgAADnF1mUaSVq5cqXOnTunTZs2aWBgQBUVFers7Bxf1Hr69GkVFFzMOCMjI3rggQf07rvv6oorrtCiRYv0gx/8QCtXrkzfT2FXQ4NUX0+HCAAAOcB2z4gT0lp6BgAAsiIjPSMAAADpRhgBAACOIowAAABHEUYAAICjCCMAAMBRhBEAAOAowggAAHAUYQQAADiKMAIAABxluw7eCdGS2KGhIYdHAgAAkhX93J6q7N0VYWR4eFiSFAwGHR4JAACwa3h4WHPnzk349654Nk0kEtF7772n2bNny+fzpe19h4aGFAwG1dfXxzNvsoDznV2c7+zifGcX5zu7Uj3fpmlqeHhY8+fPj3mI7uVcMTNSUFCg8vLyjL3/nDlz+GXOIs53dnG+s4vznV2c7+xK5XxPNiMSxQJWAADgKMIIAABwVF6HkUAgoLa2NgUCAaeHkhc439nF+c4uznd2cb6zK9Pn2xULWAEAgHfl9cwIAABwHmEEAAA4ijACAAAcRRgBAACO8nwY2b59u0KhkIqKilRVVaXDhw9Puv+LL76oRYsWqaioSDfffLMOHDiQpZF6g53z3dHRoWXLlumqq67SVVddpbq6uin//4NYdn+/o/bs2SOfz6e77rorswP0GLvn+4MPPtCGDRtUVlamQCCg6667jv+m2GD3fG/dulXXX3+9rrjiCgWDQT300EP6/e9/n6XRutt//ud/asWKFZo/f758Pp9++tOfTnlMd3e3Pve5zykQCOjaa6/V888/n/oATA/bs2ePWVhYaO7atcv8zW9+Y65bt8688sorzcHBwbj7/+pXvzL9fr/57W9/23zzzTfNRx55xJw5c6b5xhtvZHnk7mT3fDc2Nprbt283X3/9dbOnp8f8y7/8S3Pu3Lnmu+++m+WRu5Pd8x3V29trLliwwFy2bJlZX1+fncF6gN3zPTo6ai5ZssS84447zFdffdXs7e01u7u7zWPHjmV55O5k93z/8Ic/NAOBgPnDH/7Q7O3tNX/xi1+YZWVl5kMPPZTlkbvTgQMHzIcfftgMh8OmJPMnP/nJpPufPHnSnDVrltnS0mK++eab5rPPPmv6/X6zs7Mzpe/v6TCydOlSc8OGDeN/NgzDnD9/vtne3h53/3vuuce88847Y7ZVVVWZf/3Xf53RcXqF3fN9uY8//ticPXu2uXv37kwN0VNSOd8ff/yxeeutt5r/+q//aq5Zs4YwYoPd8/3d737XvOaaa8yxsbFsDdFT7J7vDRs2mH/6p38as62lpcW87bbbMjpOL0omjPz93/+9+ZnPfCZm28qVK83ly5en9D09e5lmbGxMR44cUV1d3fi2goIC1dXV6dChQ3GPOXToUMz+krR8+fKE++OiVM735T788EN99NFHuvrqqzM1TM9I9Xxv3rxZ8+bN09e+9rVsDNMzUjnfP/vZz1RdXa0NGzaopKREN910k7Zs2SLDMLI1bNdK5XzfeuutOnLkyPilnJMnT+rAgQO64447sjLmfJPuz0tXPCgvFefPn5dhGCopKYnZXlJSorfeeivuMQMDA3H3HxgYyNg4vSKV8325b37zm5o/f/6EX3BMlMr5fvXVV7Vz504dO3YsCyP0llTO98mTJ/XLX/5SX/nKV3TgwAGdOHFCDzzwgD766CO1tbVlY9iulcr5bmxs1Pnz5/X5z39epmnq448/1t/8zd/oW9/6VjaGnHcSfV4ODQ3pf//3f3XFFVfYej/PzozAXZ588knt2bNHP/nJT1RUVOT0cDxneHhYq1atUkdHh4qLi50eTl6IRCKaN2+evve976myslIrV67Uww8/rB07djg9NE/q7u7Wli1b9J3vfEdHjx5VOBzW/v379fjjjzs9NCTBszMjxcXF8vv9GhwcjNk+ODio0tLSuMeUlpba2h8XpXK+o55++mk9+eST+o//+A999rOfzeQwPcPu+f7tb3+rU6dOacWKFePbIpGIJGnGjBk6fvy4Fi5cmNlBu1gqv99lZWWaOXOm/H7/+LYbbrhBAwMDGhsbU2FhYUbH7GapnO9HH31Uq1at0n333SdJuvnmmzUyMqL7779fDz/8sAoK+Ld3OiX6vJwzZ47tWRHJwzMjhYWFqqysVFdX1/i2SCSirq4uVVdXxz2muro6Zn9JevnllxPuj4tSOd+S9O1vf1uPP/64Ojs7tWTJkmwM1RPsnu9FixbpjTfe0LFjx8Zff/Znf6ba2lodO3ZMwWAwm8N3nVR+v2+77TadOHFiPPRJ0ttvv62ysjKCyBRSOd8ffvjhhMARDYImj2BLu7R/Xqa07NUl9uzZYwYCAfP5558333zzTfP+++83r7zySnNgYMA0TdNctWqVuXHjxvH9f/WrX5kzZswwn376abOnp8dsa2vj1l4b7J7vJ5980iwsLDT37dtnnjlzZvw1PDzs1I/gKnbP9+W4m8Yeu+f79OnT5uzZs82vf/3r5vHjx82f//zn5rx588x//Md/dOpHcBW757utrc2cPXu2+cILL5gnT540X3rpJXPhwoXmPffc49SP4CrDw8Pm66+/br7++uumJPOZZ54xX3/9dfOdd94xTdM0N27caK5atWp8/+itvX/3d39n9vT0mNu3b+fW3sk8++yz5qc+9SmzsLDQXLp0qflf//Vf4393++23m2vWrInZ/8c//rF53XXXmYWFheZnPvMZc//+/VkesbvZOd+f/vSnTUkTXm1tbdkfuEvZ/f2+FGHEPrvn+7XXXjOrqqrMQCBgXnPNNeYTTzxhfvzxx1ketXvZOd8fffSR+dhjj5kLFy40i4qKzGAwaD7wwAPm+++/n/2Bu9Arr7wS97/H0XO8Zs0a8/bbb59wTEVFhVlYWGhec8015r/927+l/P19psn8FQAAcI5n14wAAAB3IIwAAABHEUYAAICjCCMAAMBRhBEAAOAowggAAHAUYQQAADiKMAIAABxFGAEAAI4ijAAAAEcRRgAAgKMIIwAAwFH/H7pGcP9So8n5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.scatter(x_train,y_train , color=\"red\")\n",
    "\n",
    "plt.scatter(x_test,y_preds , color=\"green\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params = model_0.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (40) must match the size of tensor b (10) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m     32\u001b[0m     test_pred \u001b[39m=\u001b[39m model_0(x_test)\n\u001b[0;32m---> 33\u001b[0m     test_loss \u001b[39m=\u001b[39m loss_fn(y_pred , y_test\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39;49mfloat32))\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m epoch\u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     35\u001b[0m         epoch_count\u001b[39m.\u001b[39mappend(epoch)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:101\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49ml1_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:3260\u001b[0m, in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3257\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3258\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3260\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   3261\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39ml1_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (40) must match the size of tensor b (10) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "#now since we see that the above weight and bias was  a random vector  ,we now want to train and then test the model \n",
    "#for this we will create the training loop \n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_0.train()\n",
    "    #training means perform the forward pass  , \n",
    "    #after doing the forward pass , now we try to do the loss calculation \n",
    "    y_pred = model_0(x_train)\n",
    "    loss = loss_fn(y_train , y_pred)\n",
    "     #after calculating the loss , we now want to backpropagate \n",
    "    #firstly we zero the gradients \n",
    "    optimizer.zero_grad()\n",
    "    #we perform the backpropagation \n",
    "    loss.backward()\n",
    "    \n",
    "    #perfom the optimizer step \n",
    "    optimizer.step()\n",
    "    \n",
    "    #then we put the model in the eval mode\n",
    "    model_0.eval()\n",
    "    \n",
    "    #now we need to test the model \n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_0(x_test)\n",
    "        test_loss = loss_fn(y_pred , y_test.type(torch.float32))\n",
    "        if epoch% 10 ==0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            \n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss : {loss} | MAE Test loss : {test_loss}\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LinearRegressionModel_v2(\n",
       "   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
       " ),\n",
       " OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n",
       "              ('linear_layer.bias', tensor([0.8300]))]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets create a second version of the linear regression model \n",
    "class LinearRegressionModel_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_features = 1 , out_features = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "#this has created the model \n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_1 = LinearRegressionModel_v2()\n",
    "\n",
    "model_1, model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the tensors are in cpu , we need to send them to gpu \n",
    "model_1.to(\"mps\")\n",
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we create the loss function \n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params = model_1.parameters(), lr = 0.01)\n",
    "device= \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([40, 1])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (40) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m     26\u001b[0m     y_preds \u001b[39m=\u001b[39m model_1(x_test)\n\u001b[0;32m---> 28\u001b[0m     test_loss  \u001b[39m=\u001b[39m loss_fn(y_test,y_pred)\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m epoch\u001b[39m%\u001b[39m\u001b[39m100\u001b[39m \u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m | Train loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m | Test loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:101\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49ml1_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:3260\u001b[0m, in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3257\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3258\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3260\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   3261\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39ml1_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (40) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "#after setting the necessary loss and optimizer function , we need to now build the training loop \n",
    "\n",
    "torch.manual_seed(42)\n",
    "epochs = 1000\n",
    "\n",
    "#since the parameters of the model are in gpu , we need to ttransfer all the lists to gpu\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_1.train()\n",
    "    \n",
    "    y_pred = model_1(x_train)\n",
    "    \n",
    "    loss = loss_fn(y_pred , y_train )\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_preds = model_1(x_test)\n",
    "        \n",
    "        test_loss  = loss_fn(y_test,y_pred)\n",
    "        \n",
    "    if epoch%100 ==0:\n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
