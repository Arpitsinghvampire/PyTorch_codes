{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXwUCS1drGHTFpnxtHJ5kf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpitsinghvampire/PyTorch_codes/blob/main/TRANSFORMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C1JGK74XCQVf"
      },
      "outputs": [],
      "source": [
        "#we use  transforms to perform some manipulation of teh data and make it suitable for training\n",
        "\n",
        "#all torchvision  datasets hasd two parameters  -transform to modify features\n",
        "#-target_transform to modify the labels  (modify target vectors)\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor , Lambda\n",
        "from torch import nn\n",
        "\n",
        "dataset = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        "    target_transform = Lambda(lambda y: torch.zeros(10, dtype = torch.float).scatter_(0,torch.tensor(y) , value = 1)) #this basically does  one hot encoded part\n",
        ")\n",
        "\n",
        "dataset = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        "    target_transform = Lambda(lambda y: torch.zeros(10, dtype = torch.float).scatter_(0,torch.tensor(y) , value = 1)) #this basically does  one hot encoded part\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_module = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Sequential(\n",
        "        nn.Linear(28*28 , 200),\n",
        "        nn.ReLU() ,\n",
        "        nn.Linear(200,100)\n",
        "    ),\n",
        "    nn.Linear(100,10)\n",
        ")\n",
        "\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_module(input_image)"
      ],
      "metadata": {
        "id": "fPh9jNQFIfoJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.SOFTMAX"
      ],
      "metadata": {
        "id": "3JUos1EtKq9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim = 1) #the column sum should sum upto 1\n",
        "pred_proba = softmax(logits) #applies softmax to the logits\n",
        "\n",
        "\n",
        "#now lets plot the models\n",
        "print(f\"Model Structure {seq_module} \\n\")\n",
        "for name , param in seq_module.named_parameters():\n",
        "    print(f\"Layer: {name} , Size = {param.size()} , values = {param[:2]} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaEPsCfJKtqR",
        "outputId": "ffd67538-cf8f-437f-ea3e-115cee9c262f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Structure Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
            "  )\n",
            "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
            ") \n",
            "\n",
            "Layer: 1.0.weight , Size = torch.Size([200, 784]) , values = tensor([[-0.0114,  0.0007,  0.0291,  ...,  0.0344, -0.0128,  0.0287],\n",
            "        [ 0.0087, -0.0356, -0.0270,  ..., -0.0224, -0.0263, -0.0332]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: 1.0.bias , Size = torch.Size([200]) , values = tensor([-0.0094,  0.0081], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: 1.2.weight , Size = torch.Size([100, 200]) , values = tensor([[ 0.0680, -0.0473, -0.0173,  0.0424,  0.0323,  0.0348,  0.0413, -0.0577,\n",
            "         -0.0159,  0.0066,  0.0308, -0.0581, -0.0564, -0.0605,  0.0045,  0.0623,\n",
            "         -0.0702,  0.0118, -0.0457, -0.0410,  0.0659, -0.0573,  0.0531, -0.0029,\n",
            "          0.0674, -0.0411, -0.0403, -0.0109, -0.0419,  0.0001,  0.0027,  0.0359,\n",
            "          0.0516, -0.0144,  0.0593,  0.0161, -0.0524, -0.0512,  0.0467, -0.0104,\n",
            "          0.0410,  0.0217, -0.0472, -0.0318, -0.0356, -0.0673, -0.0093,  0.0577,\n",
            "          0.0579, -0.0262, -0.0166,  0.0247, -0.0452, -0.0368, -0.0657, -0.0687,\n",
            "          0.0040,  0.0517,  0.0363, -0.0006,  0.0032, -0.0507, -0.0555,  0.0235,\n",
            "          0.0559, -0.0506, -0.0571, -0.0655,  0.0103,  0.0001,  0.0414,  0.0105,\n",
            "          0.0707, -0.0556, -0.0707, -0.0472,  0.0291, -0.0122,  0.0248, -0.0597,\n",
            "         -0.0096,  0.0097, -0.0281, -0.0606, -0.0600,  0.0006,  0.0149,  0.0538,\n",
            "         -0.0180, -0.0290,  0.0651,  0.0115, -0.0214,  0.0520, -0.0495,  0.0116,\n",
            "          0.0554, -0.0065, -0.0324,  0.0677,  0.0322,  0.0226,  0.0370, -0.0264,\n",
            "         -0.0656,  0.0685, -0.0545,  0.0119,  0.0214,  0.0095,  0.0206, -0.0434,\n",
            "         -0.0403,  0.0192,  0.0485,  0.0556, -0.0197, -0.0539, -0.0619,  0.0111,\n",
            "          0.0547, -0.0032,  0.0251,  0.0461, -0.0262,  0.0673, -0.0496, -0.0390,\n",
            "          0.0115, -0.0252, -0.0336,  0.0405, -0.0301, -0.0492, -0.0152, -0.0643,\n",
            "          0.0111, -0.0374,  0.0574, -0.0388,  0.0474, -0.0019,  0.0073, -0.0104,\n",
            "          0.0525,  0.0670, -0.0577, -0.0512, -0.0087, -0.0593,  0.0302,  0.0459,\n",
            "         -0.0313, -0.0646,  0.0155, -0.0436,  0.0270,  0.0183,  0.0036, -0.0279,\n",
            "          0.0305,  0.0513,  0.0443,  0.0382, -0.0523, -0.0267,  0.0601,  0.0357,\n",
            "         -0.0445,  0.0435,  0.0542, -0.0407,  0.0018, -0.0070,  0.0015, -0.0449,\n",
            "         -0.0342, -0.0603, -0.0397,  0.0504, -0.0397, -0.0223, -0.0561,  0.0170,\n",
            "         -0.0407, -0.0449,  0.0227, -0.0013,  0.0028,  0.0591, -0.0462, -0.0087,\n",
            "          0.0283, -0.0380,  0.0439, -0.0600,  0.0363,  0.0206,  0.0246, -0.0172],\n",
            "        [ 0.0072,  0.0529, -0.0172, -0.0083, -0.0053, -0.0660, -0.0173,  0.0552,\n",
            "         -0.0400, -0.0411, -0.0439, -0.0407, -0.0218,  0.0293, -0.0149, -0.0052,\n",
            "          0.0037,  0.0157,  0.0343, -0.0345,  0.0059, -0.0398,  0.0454, -0.0546,\n",
            "          0.0212, -0.0490, -0.0421, -0.0344,  0.0159, -0.0490, -0.0682,  0.0260,\n",
            "         -0.0025, -0.0221, -0.0476, -0.0619, -0.0614,  0.0680, -0.0406, -0.0410,\n",
            "          0.0123,  0.0501,  0.0577, -0.0299, -0.0520,  0.0564, -0.0439, -0.0123,\n",
            "          0.0130, -0.0607,  0.0157, -0.0405, -0.0339, -0.0457, -0.0192,  0.0178,\n",
            "         -0.0021,  0.0376, -0.0609, -0.0647,  0.0426,  0.0547, -0.0395,  0.0043,\n",
            "          0.0610, -0.0581, -0.0087, -0.0208, -0.0172,  0.0689,  0.0499, -0.0576,\n",
            "          0.0352,  0.0558,  0.0125, -0.0395,  0.0224,  0.0289, -0.0372, -0.0074,\n",
            "         -0.0649,  0.0643, -0.0009, -0.0154, -0.0557, -0.0300,  0.0291, -0.0611,\n",
            "          0.0033, -0.0490, -0.0008, -0.0602,  0.0480,  0.0447, -0.0117,  0.0610,\n",
            "          0.0506,  0.0370, -0.0648, -0.0018,  0.0258, -0.0579,  0.0142, -0.0663,\n",
            "         -0.0008,  0.0376, -0.0242,  0.0510, -0.0485,  0.0117,  0.0596, -0.0158,\n",
            "          0.0210,  0.0017, -0.0145, -0.0004,  0.0138,  0.0489, -0.0508, -0.0601,\n",
            "         -0.0483,  0.0160, -0.0523,  0.0571,  0.0186, -0.0310, -0.0099,  0.0311,\n",
            "          0.0386, -0.0317, -0.0441, -0.0577,  0.0635, -0.0068,  0.0654,  0.0690,\n",
            "         -0.0248, -0.0401, -0.0276, -0.0033,  0.0064, -0.0293, -0.0430,  0.0174,\n",
            "          0.0394,  0.0121,  0.0255,  0.0197, -0.0665, -0.0295,  0.0702,  0.0500,\n",
            "         -0.0595, -0.0485, -0.0467, -0.0590, -0.0402,  0.0197, -0.0142,  0.0460,\n",
            "         -0.0224, -0.0062, -0.0053, -0.0111, -0.0014,  0.0558,  0.0397,  0.0448,\n",
            "         -0.0503,  0.0036,  0.0138, -0.0181, -0.0566,  0.0144,  0.0690,  0.0063,\n",
            "         -0.0390, -0.0530, -0.0641, -0.0341, -0.0014,  0.0660,  0.0529, -0.0399,\n",
            "          0.0560,  0.0200, -0.0345,  0.0058, -0.0639,  0.0483,  0.0349,  0.0430,\n",
            "         -0.0676, -0.0119, -0.0365,  0.0269, -0.0703, -0.0502,  0.0283,  0.0209]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: 1.2.bias , Size = torch.Size([100]) , values = tensor([ 0.0611, -0.0008], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: 2.weight , Size = torch.Size([10, 100]) , values = tensor([[-0.0013, -0.0555, -0.0508,  0.0125,  0.0774,  0.0073,  0.0838,  0.0517,\n",
            "          0.0796,  0.0052, -0.0635,  0.0853, -0.0087,  0.0572,  0.0167,  0.0957,\n",
            "          0.0348,  0.0518,  0.0775, -0.0311, -0.0124,  0.0330, -0.0417, -0.0700,\n",
            "         -0.0895,  0.0580, -0.0505,  0.0699,  0.0323,  0.0387,  0.0793, -0.0386,\n",
            "         -0.0986,  0.0885,  0.0177, -0.0952, -0.0631, -0.0028, -0.0992,  0.0802,\n",
            "          0.0186,  0.0292, -0.0470,  0.0589, -0.0273, -0.0782, -0.0909,  0.0354,\n",
            "          0.0077,  0.0070, -0.0418, -0.0794,  0.0560,  0.0500,  0.0012, -0.0828,\n",
            "          0.0989, -0.0983, -0.0990, -0.0841,  0.0264, -0.0592,  0.0497,  0.0657,\n",
            "          0.0797, -0.0416,  0.0020, -0.0671, -0.0384,  0.0583, -0.0213, -0.0398,\n",
            "          0.0421,  0.0034,  0.0850, -0.0280,  0.0536, -0.0319,  0.0079,  0.0332,\n",
            "          0.0017, -0.0175,  0.0051, -0.0274, -0.0606, -0.0145,  0.0878, -0.0378,\n",
            "         -0.0048,  0.0858, -0.0294, -0.0318, -0.0384, -0.0165,  0.0886,  0.0288,\n",
            "         -0.0978, -0.0182,  0.0677,  0.0950],\n",
            "        [-0.0348,  0.0202, -0.0248, -0.0922,  0.0901,  0.0764,  0.0577,  0.0885,\n",
            "          0.0237,  0.0132,  0.0955, -0.0082, -0.0167, -0.0683,  0.0225, -0.0810,\n",
            "          0.0736, -0.0933, -0.0637,  0.0070, -0.0224,  0.0271,  0.0955,  0.0686,\n",
            "          0.0256, -0.0573,  0.0009,  0.0675,  0.0064,  0.0901,  0.0168, -0.0485,\n",
            "          0.0353,  0.0765,  0.0760,  0.0470, -0.0020, -0.0906,  0.0819, -0.0650,\n",
            "         -0.0284,  0.0193,  0.0576, -0.0892,  0.0192,  0.0324,  0.0977, -0.0930,\n",
            "          0.0317, -0.0507,  0.0230, -0.0610,  0.0157, -0.0050, -0.0807, -0.0694,\n",
            "          0.0544, -0.0461,  0.0712,  0.0526,  0.0530, -0.0990,  0.0929,  0.0936,\n",
            "          0.0886,  0.0259, -0.0130, -0.0319,  0.0112, -0.0411, -0.0744, -0.0187,\n",
            "         -0.0955, -0.0574, -0.0388, -0.0069,  0.0588,  0.0483,  0.0647, -0.0074,\n",
            "          0.0940, -0.0308,  0.0819,  0.0056, -0.0502, -0.0509, -0.0768, -0.0814,\n",
            "          0.0392,  0.0253, -0.0535, -0.0004, -0.0636,  0.0499,  0.0573,  0.0796,\n",
            "         -0.0537,  0.0270,  0.0990, -0.0676]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: 2.bias , Size = torch.Size([10]) , values = tensor([-0.0110, -0.0803], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUTOMATIC DIFFERENTIATION WITH torch.autograd"
      ],
      "metadata": {
        "id": "yCqSwdZHMmtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)\n",
        "y = torch.zeros(3)\n",
        "w = torch.randn(5 , 3 , requires_grad = True)\n",
        "b = torch.randn(3,requires_grad = True)\n",
        "\n",
        "z = torch.matmul(x , w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNugm5grMhJm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")\n",
        "\n",
        "loss.backward(retain_graph = True)\n",
        "\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15qRmQpmPPp0",
        "outputId": "4e619093-5587-4b36-9e6a-3b4b113f4de2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7c4c32148f40>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7c4c32148700>\n",
            "tensor([[0.2815, 0.0065, 0.3312],\n",
            "        [0.2815, 0.0065, 0.3312],\n",
            "        [0.2815, 0.0065, 0.3312],\n",
            "        [0.2815, 0.0065, 0.3312],\n",
            "        [0.2815, 0.0065, 0.3312]])\n",
            "tensor([0.2815, 0.0065, 0.3312])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#by default gradient tracking is always on\n",
        "z = torch.matmul(x,w) + b\n",
        "print(z.requires_grad)\n",
        "#in some cases we do not want to track the gradients accumulated so far , we might want to get the predictions of the current model , in that case\n",
        "#we do not want the gradients  thus we stop the gradient tracking\n",
        "with torch.no_grad():\n",
        "  z = torch.matmul(x,w)+b\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDUjCQk7PXA7",
        "outputId": "fadef07e-bf95-4ec9-9a3e-1f7c39fcd1d2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#another way to get teh same result is known as detach()  method on the tensor\n",
        "z = torch.matmul(x,w) + b\n",
        "z_detached = z.detach() #stop the gradient tracking\n",
        "print(z_detached.requires_grad)\n",
        "\n",
        "#the backward pas kicks off when .backward() is called on the DAG root .Autograd then\n",
        "\n",
        "#computes the gradient from each  gradient_fn\n",
        "#accumulates thme in ot respective tesnors .grad  attribute\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEhIfF-xPun4",
        "outputId": "ce6090ad-1811-461d-ebee-058eeb5cd4dd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inp = torch.eye(4,5 , requires_grad = True) #the gradient is tracked for all variables where the requires_grad part is True\n",
        "out = (inp+1).pow(2).t()\n",
        "#.backward() takes in a  scalar value  , but since we have a matrix , the pytorch needs to know how much each element contributes to the graph\n",
        "#torch.ones means that all the elements contribute equally\n",
        "#sum all gradients into inp.grad\n",
        "out.backward(torch.ones_like(out) , retain_graph = True) #this takes the  gradient of the output\n",
        "#this basically gives how much importance to be given to each location of the matrix  , finds the gradient of input\n",
        "print(\"FIRST TIME GRADIENT\")\n",
        "print(inp.grad)\n",
        "\n",
        "#now we do this one more time\n",
        "out.backward(torch.ones_like(out) , retain_graph = True)\n",
        "print(\"SECOND TIME  GRADIENT\")\n",
        "print(inp.grad)\n",
        "\n",
        "print(\"GRADIENT AFTER ZEROING THE GRADIENT\")\n",
        "#now lets give the gradient as 0\n",
        "inp.grad.zero_()\n",
        "#this makes the gradient 0\n",
        "#Now lets print the gradient\n",
        "print(inp.grad)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usk9yqJbR7pi",
        "outputId": "57e6f72c-b815-441f-d650-92cd6ff6025a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIRST TIME GRADIENT\n",
            "tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.]])\n",
            "SECOND TIME  GRADIENT\n",
            "tensor([[8., 4., 4., 4., 4.],\n",
            "        [4., 8., 4., 4., 4.],\n",
            "        [4., 4., 8., 4., 4.],\n",
            "        [4., 4., 4., 8., 4.]])\n",
            "GRADIENT AFTER ZEROING THE GRADIENT\n",
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that when we call backward for the second time with the same argument, the value of the gradient is different. This happens because when doing backward propagation, PyTorch accumulates the gradients, i.e. the value of computed gradients is added to the grad property of all leaf nodes of computational graph. If you want to compute the proper gradients, you need to zero out the grad property before. In real-life training an optimizer helps us to do this.\n",
        "\n"
      ],
      "metadata": {
        "id": "6DBNak3eXFfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously we were calling backward() function without any parameters .This is equivalent to  backward(torch.tensor(1.0)) , whiuch is used for calaculating the gradients  in case of a scalar valued function ."
      ],
      "metadata": {
        "id": "A0PPUG7FXHsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now lets use and load the model\n"
      ],
      "metadata": {
        "id": "58A9LWXLhfNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.vision.models as models  #from here we will use the torch vision models\n",
        "\n",
        "model = models.vgg(weights = \"IMAGENET1K_V1\")\n",
        "#now we save the models\n",
        "torch.save(model.state_dict() , 'model_weights.pth')"
      ],
      "metadata": {
        "id": "dJao4Pb5X8iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vgg16()\n",
        "model.load_state_dict(torch.load('model_weights.pth') , weights_only = True)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "evP8rwCRiYwV",
        "outputId": "5bb7716b-4df4-4429-9084-b068fa70d005"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'models' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-216426318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights.pth'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ]
    }
  ]
}